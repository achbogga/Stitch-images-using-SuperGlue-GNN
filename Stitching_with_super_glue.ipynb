{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "157dbad6-27e2-48f4-8ba6-4b38a83266ff",
   "metadata": {},
   "source": [
    "## Panorama Stitching using SuperGlue\n",
    "SuperGlue network is a Graph Neural Network combined with an Optimal Matching layer that is trained to perform matching on two sets of sparse image features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c43837a-4132-4279-aed8-a665665751f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import torch\n",
    "from models.matching import Matching\n",
    "from models.utils import (compute_pose_error, compute_epipolar_error,\n",
    "                          estimate_pose, make_matching_plot,\n",
    "                          error_colormap, AverageTimer, pose_auc, read_image,\n",
    "                          rotate_intrinsics, rotate_pose_inplane,\n",
    "                          scale_intrinsics)\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99471bda-d938-438b-8c2c-f8c72b08f2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded SuperPoint model\n",
      "Loaded SuperGlue model (\"indoor\" weights)\n"
     ]
    }
   ],
   "source": [
    "# generating the necessary txt file to input for the super glue algorithm\n",
    "input_folder = '/home/aboggaram/Desktop/Orchard_Robotics_challenge/EvaluationSetRGB'\n",
    "image_names = sorted(os.listdir(input_folder))\n",
    "image_paths = [os.path.join(input_folder, name) for name in image_names]\n",
    "nms_radius = 5\n",
    "keypoint_threshold = 0.05\n",
    "max_keypoints = 2048\n",
    "superglue = 'outdoor'\n",
    "sinkhorn_iterations = 20\n",
    "match_threshold = 0.2\n",
    "\n",
    "config = {\n",
    "        'superpoint': {\n",
    "            'nms_radius': nms_radius,\n",
    "            'keypoint_threshold': keypoint_threshold,\n",
    "            'max_keypoints': max_keypoints\n",
    "        },\n",
    "        'superglue': {\n",
    "            'weights': superglue,\n",
    "            'sinkhorn_iterations': sinkhorn_iterations,\n",
    "            'match_threshold': match_threshold,\n",
    "        }\n",
    "    }\n",
    "matching = Matching(config).eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "74d3168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame2tensor(frame, device):\n",
    "    return torch.from_numpy(frame/255.).float()[None, None].to(device)\n",
    "\n",
    "\n",
    "def preprocess_image(path, device):\n",
    "    image = cv.imread(str(path), cv.IMREAD_COLOR)\n",
    "    gray = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "    if image is None:\n",
    "        return None, None, None\n",
    "    w, h = image.shape[1], image.shape[0]\n",
    "    gray = gray.astype('float32')\n",
    "    inp = frame2tensor(gray, device)\n",
    "    return image, inp\n",
    "\n",
    "\n",
    "def get_matches(inp0, inp1, matching=matching):\n",
    "\t# Perform the matching.\n",
    "\tpred = matching({'image0': inp0, 'image1': inp1})\n",
    "\tpred = {k: v[0].cpu().numpy() for k, v in pred.items()}\n",
    "\tkpts0, kpts1 = pred['keypoints0'], pred['keypoints1']\n",
    "\tmatches, conf = pred['matches0'], pred['matching_scores0']\n",
    "\tout_matches = {'keypoints0': kpts0, 'keypoints1': kpts1,\n",
    "\t\t\t\t\t'matches': matches, 'match_confidence': conf}\n",
    "\treturn out_matches\n",
    "\n",
    "\n",
    "def stitch_images(left_image, right_image, matches):\n",
    "    # Get the keypoints from the matches\n",
    "    kpts0 = matches['keypoints0']\n",
    "    kpts1 = matches['keypoints1']\n",
    "\n",
    "    mats = matches['matches']\n",
    "    conf = matches['match_confidence']\n",
    "    # Keep the matching keypoints.\n",
    "    valid = mats > -1\n",
    "    mkpts0 = kpts0[valid]\n",
    "    mkpts1 = kpts1[mats[valid]]\n",
    "    mconf = conf[valid]\n",
    "\n",
    "    k = 50\n",
    "    # visualize the top k matches\n",
    "    topk_matches = np.argsort(mconf)[::-1][:k]\n",
    "    topk_mkpts0 = mkpts0[topk_matches]\n",
    "    topk_mkpts1 = mkpts1[topk_matches]\n",
    "\n",
    "    color = cm.jet(mconf[topk_matches])\n",
    "    text = [\n",
    "                'SuperGlue',\n",
    "                'Keypoints: {}:{}'.format(len(kpts0), len(kpts1)),\n",
    "                'Top {} Matches: {}'.format(k, len(topk_mkpts0)),\n",
    "            ]\n",
    "\n",
    "    make_matching_plot(left_image, right_image,\n",
    "                        kpts0, kpts1,\n",
    "                        topk_mkpts0, topk_mkpts1,\n",
    "                        path='matches_test.jpg',\n",
    "                        text=text,\n",
    "                        color=color,\n",
    "                        show_keypoints=False)\n",
    "\n",
    "    # Find the Homography from left to right image\n",
    "    H, status = cv.findHomography(mkpts0, mkpts1, cv.RANSAC, 5.0)\n",
    "    H_inv = np.linalg.inv(H)\n",
    "\n",
    "    # Warp the right image to align with the left image\n",
    "    h1, w1 = left_image.shape[:2]\n",
    "    h2, w2 = right_image.shape[:2]\n",
    "    result_width = w1 + w2  # Width of both images combined\n",
    "    result_height = max(h1, h2)  # Height of the taller image\n",
    "\n",
    "    warped_image = cv.warpPerspective(right_image, H_inv, (result_width, result_height))\n",
    "\n",
    "    cv.imwrite('warped_image.jpg', warped_image)\n",
    "\n",
    "\n",
    "    # Blend the images using gradient-based blending\n",
    "    blended_image = blend_images(left_image, warped_image)\n",
    "    cv.imwrite('blended_image.jpg', blended_image)\n",
    "    return blended_image\n",
    "\n",
    "\n",
    "def blend_images(image1, image2):\n",
    "    # Calculate the gradient masks for both images\n",
    "    mask1 = np.zeros_like(image1)\n",
    "    mask1[:, :image1.shape[1] // 2, :] = 1\n",
    "    mask2 = 1 - mask1\n",
    "\n",
    "    # Calculate gradients\n",
    "    gradient1 = cv.Sobel(image1, cv.CV_64F, 1, 0, ksize=5)\n",
    "    gradient2 = cv.Sobel(image2, cv.CV_64F, 1, 0, ksize=5)\n",
    "\n",
    "    # Calculate the weighted sum of the gradients\n",
    "    blended_gradient = mask1 * gradient1 + mask2 * gradient2\n",
    "\n",
    "    # Calculate the blended image\n",
    "    blended_image = image1.copy()\n",
    "    blended_image[:, image1.shape[1] // 2:, :] = image2[:, image1.shape[1] // 2:, :]\n",
    "    blended_image[:, :image1.shape[1] // 2, :] = cv.convertScaleAbs(blended_gradient)\n",
    "\n",
    "    return blended_image\n",
    "\n",
    "\n",
    "def test_image_stitching():\n",
    "    # Read and preprocess the images\n",
    "    left_image, left_input = preprocess_image(image_paths[0], device)\n",
    "    right_image, right_input = preprocess_image(image_paths[1], device)\n",
    "    # Get the matches\n",
    "    matches = get_matches(left_input, right_input)\n",
    "    # Stitch the images\n",
    "    stitched_image = stitch_images(left_image, right_image, matches)\n",
    "    # # Display the stitched image\n",
    "    # plt.figure(figsize=(20, 20))\n",
    "    # stitched_image = cv.cvtColor(stitched_image, cv.COLOR_BGR2RGB)\n",
    "    # plt.imshow(stitched_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c5f8a8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aboggaram/miniconda3/envs/yolov8/lib/python3.9/site-packages/torch/nn/functional.py:4296: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_image_stitching()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3315870c-7d76-48d9-80b2-577ef1f3bf8c",
   "metadata": {},
   "source": [
    "For each keypoint in `keypoints0`, the `matches` array indicates the index of the matching keypoint in `keypoints1`, or `-1` if the keypoint is unmatched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eda9fb-2f52-4532-90f1-88cc1c2e5529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotMatches(imageSet):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    matched_points = cv.imread('or_panorama/output/{:03}_rgb_{:03}_rgb_matches.png'.\\\n",
    "                     format(imageSet, imageSet+1),cv.IMREAD_ANYCOLOR)\n",
    "    plt.imshow(matched_points, cmap='gray', vmin = 0, vmax = 255)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b197b0-e55d-424c-bbde-61de03faf09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opencv stitcher\n",
    "import cv as cv\n",
    "import os\n",
    "stitcher = cv.Stitcher.create()\n",
    "input_folder = '/home/aboggaram/Desktop/Orchard_Robotics_challenge/EvaluationSetRGB'\n",
    "seq_image_paths = [input_folder + '/' + image for image in os.listdir(input_folder)]\n",
    "seq_images = [cv.imread(image) for image in seq_image_paths]\n",
    "status, stitched = stitcher.stitch(seq_images)\n",
    "if status == cv.STITCHER_OK:\n",
    "\tplt.figure(figsize=(10,10))\n",
    "\tstitched = cv.cvtColor(stitched, cv.COLOR_BGR2RGB)\n",
    "\tplt.imshow(stitched)\n",
    "\tplt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70caf9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv\n",
    "import numpy as np\n",
    "\n",
    "# Load the sequence of images\n",
    "image_paths = [\"image1.jpg\", \"image2.jpg\", \"image3.jpg\", ...]  # Replace with your image file paths\n",
    "images = [cv.imread(path) for path in image_paths]\n",
    "\n",
    "# Initialize an empty canvas for stitching\n",
    "stitched_image = images[0]\n",
    "\n",
    "# Iterate over the remaining images and stitch them together\n",
    "for i in range(1, len(images)):\n",
    "    # Detect and compute keypoints and descriptors\n",
    "    orb = cv.ORB_create()\n",
    "    keypoints1, descriptors1 = orb.detectAndCompute(stitched_image, None)\n",
    "    keypoints2, descriptors2 = orb.detectAndCompute(images[i], None)\n",
    "\n",
    "    # Perform feature matching\n",
    "    bf = cv.BFMatcher(cv.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(descriptors1, descriptors2)\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "    # Select the top N matches (you can adjust this number)\n",
    "    N = 50\n",
    "    matches = matches[:N]\n",
    "\n",
    "    # Extract matching keypoints from both images\n",
    "    src_pts = np.float32([keypoints1[match.queryIdx].pt for match in matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([keypoints2[match.trainIdx].pt for match in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    # Find the homography matrix\n",
    "    H, _ = cv.findHomography(src_pts, dst_pts, cv.RANSAC, 5.0)\n",
    "\n",
    "    # Warp the current image to the panorama\n",
    "    warped_image = cv.warpPerspective(images[i], H, (stitched_image.shape[1] + images[i].shape[1], images[i].shape[0]))\n",
    "\n",
    "    # Blend the images using simple averaging or other methods\n",
    "    stitched_image = cv.addWeighted(stitched_image, 0.5, warped_image, 0.5, 0)\n",
    "\n",
    "# Display or save the stitched panorama\n",
    "cv.imshow(\"Stitched Panorama\", stitched_image)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7053bd6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
